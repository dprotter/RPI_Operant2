{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "def read_file(f):\n",
    "    '''convert a csv file to a pd DataFrame. Also parse the filename into vole1/vole2/day/experiment\n",
    "    and return as a dict.'''\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    _, fname = os.path.split(f)\n",
    "    #vvvvv this should get replaced with a header in each filef\n",
    "    vole1 = fname.split('_')[0]\n",
    "    vole2 = fname.split('_')[1]\n",
    "    day = fname.split('_')[-1].split('.')[0]\n",
    "    experiment = fname.split('__')[-1].split('_day')[0]\n",
    "    return df, {'vole1':vole1, 'vole2':vole2,'day': day, 'experiment':experiment,}\n",
    "\n",
    "def analyze_file(f):\n",
    "    df, info_dict = read_file(f)\n",
    "    return analyze_df(df), info_dict\n",
    "    \n",
    "    \n",
    "def analyze_df(df):\n",
    "    counts = get_event_counts(df)\n",
    "    latencies = get_event_latencies(df)\n",
    "    \n",
    "    return [counts, latencies]\n",
    "\n",
    "def get_event_counts(df):\n",
    "    '''read a DataFrame and count the occurences of every unique event within the \"event\" column'''\n",
    "    counts = {}\n",
    "    for e in df.event.unique():\n",
    "        counts[f'{e}_count'] = len(df.loc[df.event == e])\n",
    "    return counts\n",
    "\n",
    "def get_event_latencies(df):\n",
    "    '''read a DataFrame, and get mean and median latencies for any events with associated latencies. Will not count\n",
    "    events that have no associated latencies, even if they exist for other animals. (should fix this)'''\n",
    "    e_list = df.loc[pd.notna(df.latency), 'event'].unique()\n",
    "    latencies = {}\n",
    "    for e in e_list:\n",
    "        latencies[f'{e}_latency_mean'] = round(df.loc[df.event == e, 'latency'].mean(), 5)\n",
    "        latencies[f'{e}_latency_median'] = round(df.loc[df.event == e, 'latency'].median(),5)\n",
    "        \n",
    "    return latencies\n",
    "\n",
    "def parse_file(file):\n",
    "    '''read a file, and convert it to a summary of counts, median latencies, and mean latencies.'''\n",
    "    _, fname = os.path.split(file)\n",
    "    dicts, info = analyze_file(file)\n",
    "    out = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    cols = [col for col in info.keys()]\n",
    "    vals = [info[key] for key in cols]\n",
    "    cols+=['metric', 'value','file']\n",
    "    \n",
    "    for dict in dicts:\n",
    "        for key, value in dict.items():\n",
    "            tmp = []\n",
    "            tmp+=vals\n",
    "            tmp+=[key,value, fname]\n",
    "            out+=[tmp]\n",
    "    \n",
    "    return pd.DataFrame(data = np.asarray(out), columns = cols)\n",
    "    \n",
    "def assemble_files(directory):\n",
    "    '''return a list of paths to files to parse'''\n",
    "    os.chdir(directory)\n",
    "\n",
    "    #create an empty 2d list\n",
    "    out_names = []\n",
    "\n",
    "    #this will assemble a list of ALL filenames for images, sorted by timestamp of acquisition\n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        out = [os.path.join(root, f) for f in sorted(files) if\n",
    "            f.endswith('.csv') if not 'summary' in f]\n",
    "        out_names += out\n",
    "    return out_names\n",
    "\n",
    "def parse_directory(dir):\n",
    "    '''read all files in a dir, and add to a large output dataframe'''\n",
    "    files = assemble_files(dir)\n",
    "    out_df = parse_file(files[0])\n",
    "    for file in files[1:]:\n",
    "        out_df = out_df.append(parse_file(file))\n",
    "    return out_df\n",
    "\n",
    "def summarize_directory(dir, output_file_name = None, output_file_dir = None):\n",
    "    '''use this to parse a directory and save it to a csv file. Will search deeply within a directory (IE a \n",
    "    directory of directories.)\n",
    "    dir              --> input directory (path)\n",
    "    output_file_name --> name for the summary file. default \"summary.csv\" (string)\n",
    "    output_file_dir  --> where to save the summary. Default is same dir as input (path)'''\n",
    "    \n",
    "    if output_file_dir == None:\n",
    "        output_file_dir = dir\n",
    "    if not output_file_name:\n",
    "        output_file_name = 'summary.csv'\n",
    "    output_file_path = os.path.join(output_file_dir, output_file_name)\n",
    "    \n",
    "    out_df = parse_directory(dir)\n",
    "    out_df.to_csv(output_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_directory('/Users/davidprotter/Downloads/6_06_operant_data/6_06_contingent_train_1side_day_4/')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "745cfd860df16d1ef6d2cf365f92fa0a7f3e872118162cc6e0ceace37772f4af"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 ('Computing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
